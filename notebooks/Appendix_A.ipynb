{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a77ea72-2f05-4603-b2f1-6eefe7f66e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14259449-8dac-4743-b82e-f384f858bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 — Config & Imports\n",
    "# -------------------------\n",
    "# Minimal configuration for a journal annex: fixed CRS, decay bins, paths, and a modality\n",
    "# default (MRI). Comments are concise and UK English. Keep constants explicit.\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Core settings (edit to taste) ---\n",
    "MODALITY: str = \"MRI\"              # Default modality (your project defaults to MRI)\n",
    "CRS_EPSG: int = 27700              # British National Grid for siting and spatial ops\n",
    "OD_CUTOFF_MIN: int = 60            # Car travel-time cut-off for scoring\n",
    "RANDOM_STATE: int = 42             # Reproducible clustering/seeding later on\n",
    "\n",
    "# Piecewise decay weights applied to car minutes. Zero beyond 60 by construction.\n",
    "# Each tuple is (min_inclusive, max_inclusive, weight).\n",
    "DECAY_BINS_MINUTES: Tuple[Tuple[int, int, float], ...] = (\n",
    "    (0, 10, 1.00),\n",
    "    (10, 20, 0.75),\n",
    "    (20, 30, 0.50),\n",
    "    (30, 40, 0.30),\n",
    "    (40, 50, 0.15),\n",
    "    (50, 60, 0.05),\n",
    ")\n",
    "\n",
    "# Optional exclusions (e.g., Isles of Scilly) for clustering; scoring may still include them.\n",
    "EXCLUDED_LSOAS: set[str] = {\"E06000053\"}  # Isles of Scilly\n",
    "\n",
    "# --- File paths (replace with your actual locations) ---\n",
    "# Keep paths explicit and stable for reproducibility.\n",
    "PROCESSED_DIR = Path(\"data/processed\")\n",
    "\n",
    "LSOA_PATH = PROCESSED_DIR / \"LSOA_MRI_Demand_with_Demographics_2024_v1.gpkg\"\n",
    "OD_PATH = PROCESSED_DIR / \"LSOA_to_LSOA_car_minutes_dense.csv\"  # dense for siting; trimmed at scoring\n",
    "SITES_BASELINE_PATH = PROCESSED_DIR / \"MRI_sites_capability_baseline.csv\"\n",
    "\n",
    "# --- Column standards (lightweight and explicit) ---\n",
    "LSOA_ID_COL = \"lsoa21cd\"\n",
    "OD_ORIG_COL = \"origin\"\n",
    "OD_DEST_COL = \"destination\"\n",
    "OD_MINS_COL = \"car_mins\"\n",
    "SITE_LSOA_COL = \"lsoa21cd\"         # destination LSOA of the site\n",
    "SITE_NAME_COL = \"site_name\"\n",
    "SITE_CAPACITY_COL = \"scanner_count\"  # capacity units (scanners/rooms)\n",
    "\n",
    "# Demand column candidates; first match wins. MRI default first, then fallbacks.\n",
    "DEMAND_COL_CANDIDATES = (\n",
    "    \"mri_total_demand\",\n",
    "    \"ct_total_demand\",\n",
    "    \"total_demand\",\n",
    "    \"demand\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9632d1e5-48b8-4079-9ee1-0de1adffd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Load inputs and light validation\n",
    "# -----------------------------------------\n",
    "# One guard per failure class (I/O, fields, shape, values). Silent success; clear errors.\n",
    "\n",
    "def _require_file(p: Path) -> None:\n",
    "    \"\"\"Raise a clear error if a required input file is missing.\"\"\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Required file not found: {p}\")\n",
    "\n",
    "def _select_demand_col(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Pick the first present demand column from the candidate list.\"\"\"\n",
    "    for c in DEMAND_COL_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(\n",
    "        \"No demand column found. Expected one of: \"\n",
    "        f\"{', '.join(DEMAND_COL_CANDIDATES)}\"\n",
    "    )\n",
    "\n",
    "def _normalise_lsoa_codes(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Trim and uppercase LSOA codes to avoid subtle mismatches.\"\"\"\n",
    "    return s.astype(str).str.strip().str.upper()\n",
    "\n",
    "def load_inputs(\n",
    "    lsoa_path: Path,\n",
    "    od_path: Path,\n",
    "    sites_path: Path,\n",
    "    modality: str = MODALITY,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Load baseline LSOA GeoData, dense OD matrix (car minutes), and baseline site capacity.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "            - gdf_lsoa (GeoDataFrame): LSOA geometry + demand.\n",
    "            - od (DataFrame): origin, destination, car_mins (dense; later trimmed to ≤60).\n",
    "            - sites (DataFrame): baseline site capacity by destination LSOA.\n",
    "            - demand_col (str): chosen demand column name.\n",
    "    \"\"\"\n",
    "    # --- I/O guards ---\n",
    "    _require_file(lsoa_path)\n",
    "    _require_file(od_path)\n",
    "    _require_file(sites_path)\n",
    "\n",
    "    # --- LSOA GeoData ---\n",
    "    gdf_lsoa = gpd.read_file(lsoa_path)\n",
    "    if LSOA_ID_COL not in gdf_lsoa.columns:\n",
    "        raise KeyError(f\"Missing '{LSOA_ID_COL}' in {lsoa_path.name}\")\n",
    "\n",
    "    # Normalise IDs; ensure target CRS for any later spatial work.\n",
    "    gdf_lsoa[LSOA_ID_COL] = _normalise_lsoa_codes(gdf_lsoa[LSOA_ID_COL])\n",
    "    if gdf_lsoa.crs is None:\n",
    "        raise ValueError(\"LSOA GeoDataFrame has no CRS; expected EPSG:27700.\")\n",
    "    if gdf_lsoa.crs.to_epsg() != CRS_EPSG:\n",
    "        gdf_lsoa = gdf_lsoa.to_crs(CRS_EPSG)\n",
    "\n",
    "    demand_col = _select_demand_col(gdf_lsoa)\n",
    "    # Demand must be numeric and non-negative (zeros permitted).\n",
    "    gdf_lsoa[demand_col] = pd.to_numeric(gdf_lsoa[demand_col], errors=\"coerce\")\n",
    "    if gdf_lsoa[demand_col].isna().any():\n",
    "        raise ValueError(\"Demand column contains NaNs after coercion to numeric.\")\n",
    "    if (gdf_lsoa[demand_col] < 0).any():\n",
    "        raise ValueError(\"Demand column contains negative values, which are invalid.\")\n",
    "\n",
    "    # --- OD matrix (dense; used for siting, later trimmed for scoring) ---\n",
    "    od = pd.read_csv(od_path)\n",
    "    missing_od_cols = {OD_ORIG_COL, OD_DEST_COL, OD_MINS_COL} - set(od.columns)\n",
    "    if missing_od_cols:\n",
    "        raise KeyError(f\"OD matrix missing columns: {sorted(missing_od_cols)}\")\n",
    "\n",
    "    # Normalise IDs; minutes must be numeric and ≥ 0.\n",
    "    od[OD_ORIG_COL] = _normalise_lsoa_codes(od[OD_ORIG_COL])\n",
    "    od[OD_DEST_COL] = _normalise_lsoa_codes(od[OD_DEST_COL])\n",
    "    od[OD_MINS_COL] = pd.to_numeric(od[OD_MINS_COL], errors=\"coerce\")\n",
    "\n",
    "    if od[OD_MINS_COL].isna().any() or (od[OD_MINS_COL] < 0).any():\n",
    "        raise ValueError(\"OD car minutes must be non-negative numeric values.\")\n",
    "\n",
    "    # Single shape check: OD covers the LSOA universe and includes the diagonal.\n",
    "    lsoas = set(gdf_lsoa[LSOA_ID_COL].unique())\n",
    "    od_orig = set(od[OD_ORIG_COL].unique())\n",
    "    od_dest = set(od[OD_DEST_COL].unique())\n",
    "    if not (lsoas <= od_orig and lsoas <= od_dest):\n",
    "        raise ValueError(\"OD matrix does not cover all LSOAs present in the baseline file.\")\n",
    "\n",
    "    diag = od.loc[od[OD_ORIG_COL] == od[OD_DEST_COL], [OD_ORIG_COL, OD_MINS_COL]]\n",
    "    missing_diag = lsoas - set(diag[OD_ORIG_COL].unique())\n",
    "    if missing_diag:\n",
    "        raise ValueError(\"OD matrix is missing diagonal rows for some LSOAs (self-pairs).\")\n",
    "    # Allow 0 or very small self-times; enforce non-negative handled above.\n",
    "\n",
    "    # --- Baseline site capability ---\n",
    "    sites = pd.read_csv(sites_path)\n",
    "    missing_site_cols = {SITE_LSOA_COL, SITE_CAPACITY_COL} - set(sites.columns)\n",
    "    if missing_site_cols:\n",
    "        raise KeyError(f\"Sites file missing columns: {sorted(missing_site_cols)}\")\n",
    "\n",
    "    sites[SITE_LSOA_COL] = _normalise_lsoa_codes(sites[SITE_LSOA_COL])\n",
    "    sites[SITE_CAPACITY_COL] = pd.to_numeric(sites[SITE_CAPACITY_COL], errors=\"coerce\")\n",
    "\n",
    "    if sites[SITE_CAPACITY_COL].isna().any() or (sites[SITE_CAPACITY_COL] < 0).any():\n",
    "        raise ValueError(\"Site capacity must be non-negative numeric values.\")\n",
    "\n",
    "    # Light deduplication by destination LSOA in case of accidental repeats.\n",
    "    sites = (\n",
    "        sites.groupby(SITE_LSOA_COL, as_index=False)[SITE_CAPACITY_COL]\n",
    "        .sum()\n",
    "        .rename(columns={SITE_CAPACITY_COL: SITE_CAPACITY_COL})\n",
    "    )\n",
    "\n",
    "    # Return a tidy bundle; later steps can trim OD to ≤60 and compute R_j and F_i.\n",
    "    return {\n",
    "        \"gdf_lsoa\": gdf_lsoa,\n",
    "        \"od\": od,\n",
    "        \"sites\": sites,\n",
    "        \"demand_col\": demand_col,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Usage (example; leave commented in the annex) ---\n",
    "# data = load_inputs(LSOA_PATH, OD_PATH, SITES_BASELINE_PATH, modality=MODALITY)\n",
    "# gdf_lsoa, od, sites, demand_col = (\n",
    "#     data[\"gdf_lsoa\"], data[\"od\"], data[\"sites\"], data[\"demand_col\"]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "221b18b7-d339-48a7-9c73-76749a53d37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 — Prepare OD for scoring (trim, decay, denominators)\n",
    "# -----------------------------------------------------------\n",
    "# We trim the dense OD matrix to ≤60 minutes, apply piecewise decay weights,\n",
    "# attach origin demand, and aggregate to a per-destination denominator used\n",
    "# for computing site pressure R_j in the next step.\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def weight_for_minutes(minutes: float) -> float:\n",
    "    \"\"\"\n",
    "    Return the decay weight for a given car travel time in minutes.\n",
    "    Uses inclusive bins defined in DECAY_BINS_MINUTES; zero beyond the cut-off.\n",
    "    \"\"\"\n",
    "    m = float(minutes)\n",
    "    for lo, hi, w in DECAY_BINS_MINUTES:\n",
    "        if lo <= m <= hi:\n",
    "            return float(w)\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def prepare_scoring_frames(bundle: Dict[str, object], lsoa_id_col: str = LSOA_ID_COL) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Build trimmed, decay-weighted OD and the per-destination denominator.\n",
    "\n",
    "    Args:\n",
    "        bundle: dict from load_inputs() with keys 'gdf_lsoa', 'od', 'sites', 'demand_col'.\n",
    "        lsoa_id_col: LSOA code column name (default from config).\n",
    "\n",
    "    Returns:\n",
    "        dict with:\n",
    "            - od_scoring: origin, destination, car_mins, decay_w, demand, weighted_demand (≤60 min only)\n",
    "            - site_denominator: destination LSOA with 'total_weighted_demand'\n",
    "            - sites_ready: baseline site capacity table keyed by destination LSOA\n",
    "    \"\"\"\n",
    "    gdf_lsoa: gpd.GeoDataFrame = bundle[\"gdf_lsoa\"]\n",
    "    od: pd.DataFrame = bundle[\"od\"]\n",
    "    sites: pd.DataFrame = bundle[\"sites\"]\n",
    "    demand_col: str = bundle[\"demand_col\"]\n",
    "\n",
    "    # 1) Trim to the scoring cut-off (≤60 minutes) and drop zero-weight rows.\n",
    "    od_scoring = od.loc[od[OD_MINS_COL] <= OD_CUTOFF_MIN, [OD_ORIG_COL, OD_DEST_COL, OD_MINS_COL]].copy()\n",
    "    od_scoring[\"decay_w\"] = od_scoring[OD_MINS_COL].apply(weight_for_minutes)\n",
    "    od_scoring = od_scoring.loc[od_scoring[\"decay_w\"] > 0.0].copy()\n",
    "\n",
    "    # 2) Attach origin demand and compute weighted demand.\n",
    "    origin_demand = gdf_lsoa[[lsoa_id_col, demand_col]].rename(columns={lsoa_id_col: OD_ORIG_COL})\n",
    "    od_scoring = od_scoring.merge(origin_demand, on=OD_ORIG_COL, how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "    if od_scoring[demand_col].isna().any():\n",
    "        # One clear failure if any origin demand is missing in the trimmed OD.\n",
    "        raise ValueError(\"Trimmed OD references origins with no demand value in the LSOA table.\")\n",
    "\n",
    "    od_scoring[\"weighted_demand\"] = od_scoring[demand_col] * od_scoring[\"decay_w\"]\n",
    "\n",
    "    # 3) Aggregate to per-destination denominator for R_j.\n",
    "    site_denominator = (\n",
    "        od_scoring.groupby(OD_DEST_COL, as_index=False)[\"weighted_demand\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"weighted_demand\": \"total_weighted_demand\"})\n",
    "    )\n",
    "\n",
    "    # 4) Ensure every site destination appears in the denominator table\n",
    "    #    (sites with no ≤60-minute origins get zero, so R_j becomes 0 later).\n",
    "    sites_ready = sites[[SITE_LSOA_COL, SITE_CAPACITY_COL]].copy()\n",
    "    sites_ready = (\n",
    "        sites_ready.groupby(SITE_LSOA_COL, as_index=False)[SITE_CAPACITY_COL].sum()\n",
    "        .rename(columns={SITE_LSOA_COL: OD_DEST_COL})\n",
    "    )\n",
    "\n",
    "    site_denominator = sites_ready.merge(\n",
    "        site_denominator, on=OD_DEST_COL, how=\"left\", validate=\"one_to_one\"\n",
    "    )\n",
    "    site_denominator[\"total_weighted_demand\"] = site_denominator[\"total_weighted_demand\"].fillna(0.0)\n",
    "\n",
    "    return {\n",
    "        \"od_scoring\": od_scoring,\n",
    "        \"site_denominator\": site_denominator,  # columns: destination, scanner_count, total_weighted_demand\n",
    "        \"sites_ready\": sites_ready,            # keyed by destination for any later joins\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Usage (example; leave commented in the annex) ---\n",
    "# data = load_inputs(LSOA_PATH, OD_PATH, SITES_BASELINE_PATH)\n",
    "# frames = prepare_scoring_frames(data)\n",
    "# od_scoring = frames[\"od_scoring\"]\n",
    "# site_denominator = frames[\"site_denominator\"]\n",
    "# sites_ready = frames[\"sites_ready\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5538891-1c0e-4e9b-b219-d8dc2ae06d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 — Compute site pressure (R_j) and baseline accessibility (F_i)\n",
    "# --------------------------------------------------------------------\n",
    "# R_j: site capacity divided by decay-weighted demand within ≤60 minutes.\n",
    "# F_i: sum over destinations of (decay weight × R_j) for each origin LSOA.\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "RJ_COL = \"Rj\"\n",
    "BASELINE_SCORE_COL = \"score_baseline\"\n",
    "\n",
    "\n",
    "def compute_site_pressure(site_denominator: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute site pressure R_j = capacity / total_weighted_demand.\n",
    "    Zero denominator yields R_j = 0.0 (no contribution to any F_i).\n",
    "    \"\"\"\n",
    "    df = site_denominator.copy()\n",
    "    denom = df[\"total_weighted_demand\"].to_numpy()\n",
    "    cap = df[SITE_CAPACITY_COL].to_numpy()\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        rj = np.where(denom > 0.0, cap / denom, 0.0)\n",
    "\n",
    "    df[RJ_COL] = rj.astype(float)\n",
    "    return df[[OD_DEST_COL, SITE_CAPACITY_COL, \"total_weighted_demand\", RJ_COL]]\n",
    "\n",
    "\n",
    "def compute_baseline_access(\n",
    "    bundle: Dict[str, object], frames: Dict[str, pd.DataFrame]\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Merge R_j onto the trimmed OD, sum decay-weighted contributions per origin,\n",
    "    and append the baseline score to the LSOA GeoDataFrame.\n",
    "\n",
    "    Args:\n",
    "        bundle: dict from load_inputs() with keys 'gdf_lsoa', 'demand_col', etc.\n",
    "        frames: dict from prepare_scoring_frames() with keys 'od_scoring', 'site_denominator'.\n",
    "\n",
    "    Returns:\n",
    "        dict with:\n",
    "            - rj_table (DataFrame): destination, capacity, denominator, Rj\n",
    "            - gdf_baseline (GeoDataFrame): original LSOAs + 'score_baseline'\n",
    "    \"\"\"\n",
    "    gdf_lsoa: gpd.GeoDataFrame = bundle[\"gdf_lsoa\"]\n",
    "    od_scoring: pd.DataFrame = frames[\"od_scoring\"]\n",
    "    site_denominator: pd.DataFrame = frames[\"site_denominator\"]\n",
    "\n",
    "    # 1) Site pressure R_j\n",
    "    rj_table = compute_site_pressure(site_denominator)\n",
    "\n",
    "    # 2) Join R_j to OD and sum contributions per origin\n",
    "    od_with_rj = od_scoring.merge(\n",
    "        rj_table[[OD_DEST_COL, RJ_COL]],\n",
    "        on=OD_DEST_COL,\n",
    "        how=\"left\",\n",
    "        validate=\"many_to_one\",\n",
    "    )\n",
    "\n",
    "    # Any destination with no R_j (should not happen) contributes zero\n",
    "    od_with_rj[RJ_COL] = od_with_rj[RJ_COL].fillna(0.0)\n",
    "\n",
    "    # Contribution = decay_w × R_j\n",
    "    od_with_rj[\"contrib\"] = od_with_rj[\"decay_w\"] * od_with_rj[RJ_COL]\n",
    "\n",
    "    fi = (\n",
    "        od_with_rj.groupby(OD_ORIG_COL, as_index=False)[\"contrib\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"contrib\": BASELINE_SCORE_COL})\n",
    "    )\n",
    "\n",
    "    # 3) Attach F_i back to all LSOAs (fill 0 for any with no ≤60-minute links)\n",
    "    gdf_baseline = gdf_lsoa.merge(\n",
    "        fi.rename(columns={OD_ORIG_COL: LSOA_ID_COL}),\n",
    "        on=LSOA_ID_COL,\n",
    "        how=\"left\",\n",
    "        validate=\"one_to_one\",\n",
    "    )\n",
    "    gdf_baseline[BASELINE_SCORE_COL] = gdf_baseline[BASELINE_SCORE_COL].fillna(0.0)\n",
    "\n",
    "    return {\"rj_table\": rj_table, \"gdf_baseline\": gdf_baseline}\n",
    "\n",
    "\n",
    "# --- Usage (example; leave commented in the annex) ---\n",
    "# data = load_inputs(LSOA_PATH, OD_PATH, SITES_BASELINE_PATH)\n",
    "# frames = prepare_scoring_frames(data)\n",
    "# out_baseline = compute_baseline_access(data, frames)\n",
    "# rj_table = out_baseline[\"rj_table\"]\n",
    "# gdf_baseline = out_baseline[\"gdf_baseline\"]  # contains 'score_baseline'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
